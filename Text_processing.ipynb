{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**TexT Preprocessing**\n",
        "\n",
        "It is a crucial step in NLP that occurs after Data-Cleaning before building model.It is used for cleaning and preparing raw data for better analysis and fine model performance.\n",
        "\n",
        "* **What:** Converting the text into standard form(lower case.)\n",
        "* **When:** At the beingning of text processing.\n",
        "* **Why:** To reduce variability and ensure  consistency.\n",
        "* **How:** By using string method or custom function."
      ],
      "metadata": {
        "id": "ngdl9E6c4DZ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkywT5ai4Bjz",
        "outputId": "d866b11e-c6fa-4074-a434-1538361222f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a capital of usa\n"
          ]
        }
      ],
      "source": [
        "#Text Normalization\n",
        "\n",
        "text='This iS a Capital of UsA'\n",
        "normalized_text=text.lower()\n",
        "print(normalized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regular Expression(Regex)**\n",
        "\n",
        "* **What:** Pattern based text extraction.\n",
        "* **When:** Early in the text processing pipeline.\n",
        "* **Why:** To modify specific character and words(URL,Hashtags).\n",
        "* **How:** By using `re` module in python."
      ],
      "metadata": {
        "id": "vOErU_E5Fdzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text='check out this link: http://www.facebook.com'\n",
        "clean_text=re.sub(r'http\\S+','',text)\n",
        "print(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTydolkeFYEE",
        "outputId": "b37c8d75-32e7-4a98-e43d-82745a19013b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check out this link: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "\n",
        "* **What:** Breaking down into small chunks or units.\n",
        "* **When:** After normalization and regex.\n",
        "* **Why:** To attain contextual meaning of individual words.\n",
        "* **How:** Using libraries NLTK and Scapy."
      ],
      "metadata": {
        "id": "fWhj20r3LkJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text=(\"Python is easy language\")\n",
        "tokens= word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SB_VdpyKiqA",
        "outputId": "ba65d247-8fa0-437c-9c7d-4ff2ad42ff4d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Python', 'is', 'easy', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop Wprds**\n",
        "\n",
        "* **What:** Eliminating common words carry no significant meaning.\n",
        "* **When:** After tokenization, focusing on content words important.\n",
        "* **Why:** To reduce noisey words and focusing on meaningfull words.\n",
        "* **How:** Using pre-defined lists of stop words and custom list"
      ],
      "metadata": {
        "id": "1ynYwi-4PFB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words=set(stopwords.words('english'))\n",
        "text=(\"This is a Sample text for tokenization I am learning in python\")\n",
        "tokens=word_tokenize(text)\n",
        "filterd_tokens=[word for word in tokens if word.lower() not in stop_words]\n",
        "print(filterd_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmNxN-DrMcDa",
        "outputId": "94846a3e-b01e-424d-a0a0-98aad2838cfc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sample', 'text', 'tokenization', 'learning', 'python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**\n",
        "\n",
        "* **What:** Reducing base words to its root.\n",
        "* **When:** After stopwords removal, for instance normalization preserving\n",
        "    meaningfull extraction.\n",
        "* **Why:** To reduce noisey words and focusing on meaningfull words.\n",
        "* **How:** Using rule-based algorithm like Porter and Snowball stemmer."
      ],
      "metadata": {
        "id": "M72sSBX4VTCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import PorterStemmer\n",
        "\n",
        "stemmer= PorterStemmer()\n",
        "words=['running','liking','eating','driving','beating','cheating']\n",
        "\n",
        "stemmed_words=[stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-aLd1rbS60i",
        "outputId": "8a5726ff-83a4-45fc-fe12-3a91422854fd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'like', 'eat', 'drive', 'beat', 'cheat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**\n",
        "\n",
        "* **What:** Reducing to their base words and dictionary form.\n",
        "* **When:** After stopwords removal, for instance normalization preserving\n",
        "    meaningfull extraction.\n",
        "* **Why:** To reduce words variation and improve text analysis.\n",
        "* **How:** Using morphological analysis and dictionaries."
      ],
      "metadata": {
        "id": "3nxc9kQ7YYa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer= WordNetLemmatizer()\n",
        "words=['running','liking','eating','driving','beating','cheating','correctness']\n",
        "\n",
        "lematized_words=[lemmatizer.lemmatize(word) for word in words]\n",
        "print(lematized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_21v7RWWYMY",
        "outputId": "c91fcdaa-7605-4506-f8d9-eb86d949e3e4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'liking', 'eating', 'driving', 'beating', 'cheating', 'correctness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IEn2Ua1zbMSZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
